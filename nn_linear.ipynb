{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezi7u7aJWPsE"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class nn_linear_layer:\n",
        "    \n",
        "    # linear layer.\n",
        "    # randomly initialized by creating matrix W and bias b\n",
        "    def __init__(self, input_size, output_size, std=1):\n",
        "        self.W = np.random.normal(0,std,(output_size,input_size))\n",
        "        self.b = np.random.normal(0,std,(output_size,1))\n",
        "    \n",
        "    ######\n",
        "    ## Q1\n",
        "    def forward(self,x):\n",
        "       \n",
        "        out=(np.dot(self.W,x.T)+self.b).T\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    ######\n",
        "    ## Q2\n",
        "    ## returns three parameters\n",
        "    def backprop(self,x,dLdy): #dLdy는 (B,O), dLdx는 (B,I), dLdW는 (o,i), dLdb는 (o,1)과 같아야함.x는 (b,i) w가 (o.i)\n",
        "        N=dLdy.shape[1]\n",
        "        dLdW=np.dot(dLdy.T,x)\n",
        "        dLdb=np.sum(dLdy,axis=0).reshape(1,N) ##여기 잘못됨???\n",
        "        dLdx=np.dot(dLdy,self.W)\n",
        "        return dLdW,dLdb,dLdx\n",
        "\n",
        "    def update_weights(self,dLdW,dLdb):\n",
        "\n",
        "        # parameter update\n",
        "        self.W=self.W+dLdW\n",
        "        self.b=self.b+dLdb\n",
        "\n",
        "class nn_activation_layer:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    ######\n",
        "    ## Q3\n",
        "    def forward(self,x):\n",
        "        out=1/(1+np.exp(-x))\n",
        "        return out\n",
        "    \n",
        "    ######\n",
        "    ## Q4\n",
        "    def backprop(self,x,dLdy):\n",
        "        dx=dLdy*(1.0-1/(1+np.exp(-x)))*(1/(1+np.exp(-x)))\n",
        "        return dx\n",
        "\n",
        "\n",
        "class nn_softmax_layer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    ######\n",
        "    ## Q5\n",
        "    def forward(self,x):\n",
        "        \n",
        "        exp_x=np.exp(x)\n",
        "        exp_sum_x=(np.sum(exp_x,axis=1).T)[:,np.newaxis]\n",
        "        out=exp_x/exp_sum_x\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    ######\n",
        "    ## Q6\n",
        "    def backprop(self,x,dLdy):\n",
        "        \n",
        "        exp_x = np.exp(x)                \n",
        "        prob = exp_x / np.sum(exp_x, axis =1, keepdims=True)        \n",
        "\n",
        "        dLdx = dLdy * prob + prob         \n",
        "        return dLdx\n",
        "class nn_cross_entropy_layer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    ######\n",
        "    ## Q7\n",
        "    def forward(self,x,y): #y의 shape는 (20,1)\n",
        "        \n",
        "        Loss = 0\n",
        "        log_x = np.log(x)\n",
        "        data_num = y.shape[0]\n",
        "\n",
        "        for data_idx in range(data_num):\n",
        "          Loss -= ((1-y[data_idx])*(log_x[data_idx][0]/data_num)+(y[data_idx])*(log_x[data_idx][1]/data_num))\n",
        "      \n",
        "        return Loss\n",
        "        \n",
        "    ######\n",
        "    ## Q8\n",
        "    def backprop(self,x,y):\n",
        "        Y = np.zeros((y.shape[0], 2))\n",
        "        for data_idx in range(y.shape[0]):\n",
        "          if y[data_idx] == 0:\n",
        "            Y[data_idx][0]=1\n",
        "            Y[data_idx][1]=0\n",
        "          else:\n",
        "            Y[data_idx][0]=0\n",
        "            Y[data_idx][1]=1\n",
        "\n",
        "        for data_idx in range(y.shape[0]):\n",
        "          for idx in range(2):\n",
        "            Y[data_idx][idx] = -(Y[data_idx][idx]/x[data_idx][idx])\n",
        "        \n",
        "        dLdx = Y\n",
        "        return dLdx\n",
        "\n",
        "# number of data points for each of (0,0), (0,1), (1,0) and (1,1)\n",
        "num_d=5\n",
        "\n",
        "# number of test runs\n",
        "num_test=100\n",
        "\n",
        "## Q9. Hyperparameter setting\n",
        "## learning rate (lr)and number of gradient descent steps (num_gd_step)\n",
        "## This part is not graded (there is no definitive answer).\n",
        "## You can set this hyperparameters through experiments.\n",
        "lr=0.1\n",
        "num_gd_step=2000\n",
        "\n",
        "# dataset size\n",
        "batch_size=4*num_d\n",
        "\n",
        "# number of classes is 2\n",
        "num_class=2\n",
        "\n",
        "# variable to measure accuracy\n",
        "accuracy=0\n",
        "\n",
        "# set this True if want to plot training data\n",
        "show_train_data=True\n",
        "\n",
        "# set this True if want to plot loss over gradient descent iteration\n",
        "show_loss=True\n",
        "\n",
        "################\n",
        "# create training data\n",
        "################\n",
        "\n",
        "m_d1 = (0, 0)\n",
        "m_d2 = (1, 1)\n",
        "m_d3 = (0, 1)\n",
        "m_d4 = (1, 0)\n",
        "\n",
        "sig = 0.05\n",
        "s_d1 = sig ** 2 * np.eye(2)\n",
        "\n",
        "d1 = np.random.multivariate_normal(m_d1, s_d1, num_d)\n",
        "d2 = np.random.multivariate_normal(m_d2, s_d1, num_d)\n",
        "d3 = np.random.multivariate_normal(m_d3, s_d1, num_d)\n",
        "d4 = np.random.multivariate_normal(m_d4, s_d1, num_d)\n",
        "\n",
        "# training data, and has shape (4*num_d,2)\n",
        "x_train_d = np.vstack((d1, d2, d3, d4))\n",
        "# training data lables, and has shape (4*num_d,1)\n",
        "y_train_d = np.vstack((np.zeros((2 * num_d, 1), dtype='uint8'), np.ones((2 * num_d, 1), dtype='uint8')))\n",
        "\n",
        "if (show_train_data):\n",
        "    plt.grid()\n",
        "    plt.scatter(x_train_d[range(2 * num_d), 0], x_train_d[range(2 * num_d), 1], color='b', marker='o')\n",
        "    plt.scatter(x_train_d[range(2 * num_d, 4 * num_d), 0], x_train_d[range(2 * num_d, 4 * num_d), 1], color='r',\n",
        "                marker='x')\n",
        "    plt.show()\n",
        "\n",
        "################\n",
        "# create layers\n",
        "################\n",
        "\n",
        "# hidden layer\n",
        "# linear layer\n",
        "layer1 = nn_linear_layer(input_size=2, output_size=4, )\n",
        "# activation layer\n",
        "act = nn_activation_layer()\n",
        "\n",
        "# output layer\n",
        "# linear\n",
        "layer2 = nn_linear_layer(input_size=4, output_size=2, )\n",
        "# softmax\n",
        "smax = nn_softmax_layer()\n",
        "# cross entropy\n",
        "cent = nn_cross_entropy_layer()\n",
        "\n",
        "# variable for plotting loss\n",
        "loss_out = np.zeros((num_gd_step))\n",
        "\n",
        "################\n",
        "# do training\n",
        "################\n",
        "\n",
        "for i in range(num_gd_step):\n",
        "    \n",
        "    # fetch data\n",
        "    x_train = x_train_d\n",
        "    y_train = y_train_d\n",
        "    \n",
        "    # create one-hot vectors from the ground truth labels\n",
        "    y_onehot = np.zeros((batch_size, num_class))\n",
        "    y_onehot[range(batch_size), y_train.reshape(batch_size, )] = 1\n",
        "    \n",
        "    ################\n",
        "    # forward pass\n",
        "    \n",
        "    # hidden layer\n",
        "    # linear\n",
        "    l1_out = layer1.forward(x_train)\n",
        "    \n",
        "    # activation\n",
        "    a1_out = act.forward(l1_out)\n",
        "    \n",
        "    # output layer\n",
        "    # linear\n",
        "    l2_out = layer2.forward(a1_out)\n",
        "    # softmax\n",
        "    smax_out = smax.forward(l2_out)\n",
        "    # cross entropy loss\n",
        "    loss_out[i] = cent.forward(smax_out, y_train)\n",
        "    \n",
        "    ################\n",
        "    # perform backprop\n",
        "    # output layer\n",
        "    # cross entropy\n",
        "    b_cent_out = cent.backprop(smax_out, y_train)\n",
        "    # softmax\n",
        "    b_nce_smax_out = smax.backprop(l2_out, b_cent_out)\n",
        "    \n",
        "    # linear\n",
        "    b_dLdW_2, b_dLdb_2, b_dLdx_2 = layer2.backprop(x=a1_out, dLdy=b_nce_smax_out)\n",
        "    \n",
        "    # backprop, hidden layer\n",
        "    # activation\n",
        "    b_act_out = act.backprop(x=l1_out, dLdy=b_dLdx_2)\n",
        "    # linear\n",
        "    b_dLdW_1, b_dLdb_1, b_dLdx_1 = layer1.backprop(x=x_train, dLdy=b_act_out)\n",
        "    \n",
        "    ################\n",
        "    # update weights: perform gradient descent\n",
        "    layer2.update_weights(dLdW=-b_dLdW_2 * lr, dLdb=-b_dLdb_2.T * lr)\n",
        "    layer1.update_weights(dLdW=-b_dLdW_1 * lr, dLdb=-b_dLdb_1.T * lr)\n",
        "    \n",
        "    if (i + 1) % 2000 == 0:\n",
        "        print('gradient descent iteration:', i + 1)\n",
        "    \n",
        "   \n",
        "    \n",
        "\n",
        "# set show_loss to True to plot the loss over gradient descent iterations\n",
        "if (show_loss):\n",
        "    plt.figure(1)\n",
        "    plt.grid()\n",
        "    plt.plot(range(num_gd_step), loss_out)\n",
        "    plt.xlabel('number of gradient descent steps')\n",
        "    plt.ylabel('cross entropy loss')\n",
        "    plt.show()\n",
        "\n",
        "################\n",
        "# training done\n",
        "# now testing\n",
        "\n",
        "num_test = 100\n",
        "\n",
        "for j in range(num_test):\n",
        "    \n",
        "    predicted = np.ones((4,))\n",
        "    \n",
        "    # dispersion of test data\n",
        "    sig_t = 1e-2\n",
        "    \n",
        "    # generate test data\n",
        "    # generate 4 samples, each sample nearby (1,1), (0,0), (1,0), (0,1) respectively\n",
        "    t11 = np.random.multivariate_normal((1,1), sig_t**2*np.eye(2), 1)\n",
        "    t00 = np.random.multivariate_normal((0,0), sig_t**2*np.eye(2), 1)\n",
        "    t10 = np.random.multivariate_normal((1,0), sig_t**2*np.eye(2), 1)\n",
        "    t01 = np.random.multivariate_normal((0,1), sig_t**2*np.eye(2), 1)\n",
        "    \n",
        "    # predicting label for test sample nearby (1,1)\n",
        "    l1_out = layer1.forward(t11)\n",
        "    a1_out = act.forward(l1_out)\n",
        "    l2_out = layer2.forward(a1_out)\n",
        "    smax_out = smax.forward(l2_out)\n",
        "    predicted[0] = np.argmax(smax_out)\n",
        "    print('softmax out for (1,1)', smax_out, 'predicted label:', int(predicted[0]))\n",
        "    \n",
        "    # predicting label for test sample nearby (0,0)\n",
        "    l1_out = layer1.forward(t00)\n",
        "    a1_out = act.forward(l1_out)\n",
        "    l2_out = layer2.forward(a1_out)\n",
        "    smax_out = smax.forward(l2_out)\n",
        "    predicted[1] = np.argmax(smax_out)\n",
        "    print('softmax out for (0,0)', smax_out, 'predicted label:', int(predicted[1]))\n",
        "    \n",
        "    # predicting label for test sample nearby (1,0)\n",
        "    l1_out = layer1.forward(t10)\n",
        "    a1_out = act.forward(l1_out)\n",
        "    l2_out = layer2.forward(a1_out)\n",
        "    smax_out = smax.forward(l2_out)\n",
        "    predicted[2] = np.argmax(smax_out)\n",
        "    print('softmax out for (1,0)', smax_out, 'predicted label:', int(predicted[2]))\n",
        "    \n",
        "    # predicting label for test sample nearby (0,1)\n",
        "    l1_out = layer1.forward(t01)\n",
        "    a1_out = act.forward(l1_out)\n",
        "    l2_out = layer2.forward(a1_out)\n",
        "    smax_out = smax.forward(l2_out)\n",
        "    predicted[3] = np.argmax(smax_out)\n",
        "    print('softmax out for (0,1)', smax_out, 'predicted label:', int(predicted[3]))\n",
        "    \n",
        "    print('total predicted labels:', predicted.astype('uint8'))\n",
        "    \n",
        "    accuracy += (predicted[0] == 0) & (predicted[1] == 0) & (predicted[2] == 1) & (predicted[3] == 1)\n",
        "    \n",
        "    if (j + 1) % 10 == 0:\n",
        "        print('test iteration:', j + 1)\n",
        "\n",
        "print('accuracy:', accuracy / num_test * 100, '%')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzvBMXZr_umH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0iGnDzl3L-F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}