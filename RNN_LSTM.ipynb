{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "bvpwe9ZtwkQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mfAYxhbwfWZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "sst_dataset = load_dataset('sst')\n",
        "pprint(sst_dataset['train'][0])\n",
        "print(np.shape(sst_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(sst_dataset['train'][0]['sentence'].split(' '))\n",
        "#print(len(sst_dataset['train']))\n",
        "new_vocab=['PAD', 'UNK']                    #없어도 되긴함\n",
        "for i in range(len(sst_dataset['train'])):\n",
        "  new_vocab+=list(sst_dataset['train'][i]['sentence'].split(' '))\n",
        "\n",
        "vocab_count={}\n",
        "for voca in new_vocab:\n",
        "  try:\n",
        "    vocab_count[voca]+=1\n",
        "  except:\n",
        "    vocab_count[voca]=1  \n",
        "#print(vocab_count)\n",
        "\n",
        "new_vocab2=['PAD','UNK']\n",
        "for key,value in vocab_count.items():\n",
        "  if (vocab_count[key]>=2):\n",
        "    new_vocab2.append(key)\n",
        "    #print(key)\n",
        "\n",
        "print(new_vocab2)\n",
        "print(len(new_vocab2))\n",
        "#print(len(new_vocab))                       #이게 정답\n",
        "#print(new_vocab)  \n",
        "\n",
        "new_word2id={word:id for id,word in enumerate(new_vocab2)}\n",
        "print(new_word2id['he'])"
      ],
      "metadata": {
        "id": "6ZWKkpNjwq6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two-layer MLP classification\n",
        "print(len(new_vocab2))\n",
        "class Baseline(nn.Module):\n",
        "  def __init__(self, d, length):\n",
        "    super(Baseline, self).__init__()\n",
        "    self.embedding = nn.Embedding(len(new_vocab2), d)   #임베딩테이블을 만듦. 행은 vocab 인덱스,열은 그것의 벡터를 저장 ex 1행엔 1번 vocab의 벡터가 저장 \n",
        "    self.layer = nn.Linear(d * length, d, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.class_layer = nn.Linear(d, 2, bias=True)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    emb = self.embedding(input_tensor) # [batch_size, length, d]          #https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html self.embedding(input) input의 1,3,0,,,마다 word embedding값을 output으로 함. 1,3..은 각각 3개짜리 벡터로 변환 \n",
        "    #print(self.embedding.weight.shape)\n",
        "    print(emb)\n",
        "    emb_flat = emb.view(emb.size(0), -1) # [batch_size, length*d]\n",
        "    hidden = self.relu(self.layer(emb_flat))\n",
        "    logits = self.class_layer(hidden)\n",
        "    return logits\n",
        "print(vocab)\n",
        "d = 3 # usually bigger, e.g. 128\n",
        "baseline = Baseline(d, length)\n",
        "logits = baseline(input_tensor)\n",
        "softmax = nn.Softmax(1)\n",
        "print(softmax(logits)) # probability for each class\n",
        "\n",
        "cel = nn.CrossEntropyLoss()\n",
        "label = torch.LongTensor([1]) # The ground truth label for \"hi world!\" is positive.\n",
        "print(label)\n",
        "loss = cel(logits, label) # Loss, a.k.a L\n",
        "print(loss)\n",
        "\n",
        "optimizer = torch.optim.SGD(baseline.parameters(), lr=0.1)\n",
        "optimizer.zero_grad() # reset process\n",
        "loss.backward() # compute gradients\n",
        "optimizer.step() # update parameters"
      ],
      "metadata": {
        "id": "FCUolPHOxTk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from torch import nn\n",
        "baseline = Baseline(64, 16)\n",
        "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline=baseline.to(dev)\n",
        "inputs, labels = [], []\n",
        "length = 16\n",
        "input_ = sst_dataset['train']['sentence']\n",
        "print(len(input_))\n",
        "print(len(new_vocab2))\n",
        "print(input_[0])\n",
        "for i in range(len(input_)):\n",
        "\n",
        "  input_tokens = input_[i].split(' ')\n",
        "\n",
        "  input_ids = [new_word2id[word] if word in new_word2id else 1 for word in input_tokens] # UNK if word not found\n",
        "  \n",
        "  if len(input_ids) < length:\n",
        "    input_ids = input_ids + [0] * (length - len(input_ids)) # PAD tokens at the end\n",
        "  else:\n",
        "    input_ids = input_ids[:length]\n",
        "  inputs.append(input_ids)\n",
        "  \n",
        "  \n",
        "  label=(sst_dataset['train']['label'][i]>0.5)\n",
        "  labels.append(label)\n",
        "\n",
        "input_tensor = torch.LongTensor([inputs]) # the first dimension is minibatch size\n",
        "labels_tensor=torch.LongTensor([labels])\n",
        "input_tensor=input_tensor.reshape(-1,16)\n",
        "labels_tensor=labels_tensor.reshape(-1,1)\n",
        "print(input_tensor)\n",
        "print(labels_tensor)\n",
        "\n",
        "test_inputs, test_labels = [], []\n",
        "test_input_ = sst_dataset['validation']['sentence']\n",
        "print(len(test_input_))\n",
        "print(len(new_vocab2))\n",
        "print(test_input_[0])\n",
        "for i in range(len(test_input_)):\n",
        "\n",
        "  test_input_tokens = test_input_[i].split(' ')\n",
        "\n",
        "  test_input_ids = [new_word2id[word] if word in new_word2id else 1 for word in test_input_tokens] # UNK if word not found\n",
        "  \n",
        "  if len(test_input_ids) < length:\n",
        "    test_input_ids = test_input_ids + [0] * (length - len(test_input_ids)) # PAD tokens at the end\n",
        "  else:\n",
        "    test_input_ids = test_input_ids[:length]\n",
        "  test_inputs.append(test_input_ids)\n",
        "  \n",
        "  \n",
        "  test_label=(sst_dataset['validation']['label'][i]>0.5)\n",
        "  test_labels.append(test_label)\n",
        "\n",
        "test_input_tensor = torch.LongTensor([test_inputs]) # the first dimension is minibatch size\n",
        "test_labels_tensor=torch.LongTensor([test_labels])\n",
        "test_input_tensor=test_input_tensor.reshape(-1,16)\n",
        "test_labels_tensor=test_labels_tensor.reshape(-1,1)\n",
        "print(test_input_tensor.shape)\n",
        "print(test_labels_tensor.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8dftNWYxxUt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1 \n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "dataset = TensorDataset(input_tensor, labels_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "baseline = Baseline(64,16)\n",
        "baseline=baseline.to(dev)\n",
        "cel = nn.CrossEntropyLoss()\n",
        "  # We use Adam optimizer. This allows us to not worry about learning rate\n",
        "optimizer = torch.optim.Adam(baseline.parameters(), lr=0.0001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    \n",
        "    x_train, y_train = samples[0].to(dev), samples[1].to(dev)\n",
        "    #print(samples[0].shape)\n",
        "    #print(samples[1].shape)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs=baseline(x_train)\n",
        "    print('output',outputs)\n",
        "    y_train=y_train[:,0]                                    #차원축소 해줘야함\n",
        "    loss=cel(outputs,y_train)\n",
        "    print('LOSS:',loss)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "testset = TensorDataset(test_input_tensor, test_labels_tensor)\n",
        "test_dataloader = DataLoader(testset, batch_size=16)\n",
        "correct_count=0\n",
        "total_count=0\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        x_test,y_test = data[0].to(dev), data[1].to(dev)\n",
        "        test_outputs = baseline(x_test)\n",
        "        _, preds = test_outputs.max(1)\n",
        "        y_test=y_test[:,0]   #행을 기준으로 큰 값을 갖는 인덱스 반환\n",
        "        print('y_test:',y_test)\n",
        "        print('preds',preds)\n",
        "        num_correct = (y_test == preds).long().sum()\n",
        "        total_count += x_test.size(0)\n",
        "        correct_count += num_correct\n",
        "        \n",
        "        loss = cel(test_outputs, y_test)\n",
        "        print(loss)\n",
        "        print('correct count: ',num_correct)\n",
        "        print('size:',x_test.size(0)) \n",
        "\n",
        "        \n",
        "    #print('correct count: ',correct_count)\n",
        "    #print('total:',total_count)    \n",
        "    print(\"accuracy:\",correct_count/total_count)       \n",
        "   "
      ],
      "metadata": {
        "id": "NP35ch1jxhyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN"
      ],
      "metadata": {
        "id": "CR7a--lFxnYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.2 RNN\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size,output_size):           #input은 (i,j) (16,64)로 들어온다. i번째 글자의 임베딩벡터                      #input이 3차원으로 들어오면 (batch, sequence length, embedding d) 여기선 (16,16,64)\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_linear = nn.Linear(input_size,hidden_size,bias=True)   \n",
        "    self.hidden_linear = nn.Linear(hidden_size,hidden_size, bias=True)\n",
        "    self.tanh = nn.Tanh()\n",
        "    \n",
        "\n",
        "  def forward(self, input_tensor,hidden):\n",
        "    hidden=self.tanh(self.input_linear(input_tensor)+self.hidden_linear(hidden))\n",
        "\n",
        "    return hidden\n",
        "\n",
        "\n",
        "class RNNlayer(nn.Module):\n",
        "  def __init__(self,module):\n",
        "    super(RNNlayer,self).__init__()\n",
        "    self.rnn=module\n",
        "    self.out_size=64\n",
        "\n",
        "\n",
        "  def forward(self,input_tensor):\n",
        "  \n",
        "    cur_hidden=torch.zeros(input_tensor.shape[0],input_tensor.shape[2])        #input이 3차원으로 들어오면 (batch, sequence length, embedding d) 여기선 (16,16,64)\n",
        "    if input_tensor.is_cuda:\n",
        "      cur_hidden = cur_hidden.cuda()\n",
        "\n",
        "\n",
        "    for i in range(input_tensor.shape[1]):                                      #input이 2차원으로 줄어든다.\n",
        "      cur_input=input_tensor[:,i,:]\n",
        "      cur_hidden=self.rnn(cur_input,cur_hidden)\n",
        "    return cur_hidden\n",
        "    \n",
        "\n",
        "class Classification(nn.Module):\n",
        "  def __init__(self,module,d):\n",
        "    super(Classification,self).__init__()\n",
        "    self.d=d\n",
        "    self.linear=nn.Linear(d,2,bias=True)\n",
        "    self.embedding = nn.Embedding(len(new_vocab2), d)\n",
        "    self.layer=module\n",
        "\n",
        "  def forward(self,input_tensor):\n",
        "    emb = self.embedding(input_tensor)\n",
        "    outputs=self.layer(emb)\n",
        "    outputs=self.linear(emb)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "rnn=RNN(64,64,64)\n",
        "rnn_layer=RNNlayer(rnn)\n",
        "classification=Classification(rnn_layer,64)\n",
        "\n",
        "dataset = TensorDataset(input_tensor, labels_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        " \n",
        "classification = classification.cuda()\n",
        "cel = nn.CrossEntropyLoss()\n",
        "  # We use Adam optimizer. This allows us to not worry about learning rate\n",
        "optimizer = torch.optim.Adam(baseline.parameters(), lr=0.0001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    \n",
        "    x_train, y_train = samples[0].to(dev), samples[1].to(dev)\n",
        "    #print(samples[0].shape)\n",
        "    #print(samples[1].shape)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs=classification(x_train)\n",
        "    outputs=outputs[:,0] \n",
        "    print(outputs)\n",
        "    y_train=y_train[:,0] \n",
        "    print(y_train)                                   #차원축소 해줘야함\n",
        "    loss=cel(outputs,y_train)\n",
        "    print('LOSS:',loss)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "testset = TensorDataset(test_input_tensor, test_labels_tensor)\n",
        "test_dataloader = DataLoader(testset, batch_size=16)\n",
        "correct_count=0\n",
        "total_count=0\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        x_test,y_test = data[0].to(dev), data[1].to(dev)\n",
        "        test_outputs = classification(x_test)\n",
        "        test_outputs=test_outputs[:,0] \n",
        "        _, preds = test_outputs.max(1)\n",
        "        y_test=y_test[:,0]   #행을 기준으로 큰 값을 갖는 인덱스 반환\n",
        "        print('y_test:',y_test)\n",
        "        print('preds',preds)\n",
        "        num_correct = (y_test == preds).long().sum()\n",
        "        total_count += x_test.size(0)\n",
        "        correct_count += num_correct\n",
        "        \n",
        "        loss = cel(test_outputs, y_test)\n",
        "        print(loss)\n",
        "        print('correct count: ',num_correct)\n",
        "        print('size:',x_test.size(0)) \n",
        "\n",
        "        \n",
        "    #print('correct count: ',correct_count)\n",
        "    #print('total:',total_count)    \n",
        "    print(\"accuracy:\",correct_count/total_count)       \n",
        "   \n"
      ],
      "metadata": {
        "id": "3QHdQMxLxmVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "YxJpKr3SxuNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size,output_size):           #input, hidden은 (i,j) (16,64)로 들어온다. i번째 글자의 임베딩벡터                      #input이 3차원으로 들어오면 (batch, sequence length, embedding d) 여기선 (16,16,64)\n",
        "    super(LSTM, self).__init__()\n",
        "    self.FG_input_linear = nn.Linear(input_size,hidden_size,bias=True)   \n",
        "    self.FG_hidden_linear = nn.Linear(hidden_size,hidden_size, bias=True)\n",
        "\n",
        "    self.IG_input_linear = nn.Linear(input_size,hidden_size,bias=True)   \n",
        "    self.IG_hidden_linear = nn.Linear(hidden_size,hidden_size, bias=True)\n",
        "\n",
        "    self.OG_input_linear = nn.Linear(input_size,hidden_size,bias=True)   \n",
        "    self.OG_hidden_linear = nn.Linear(hidden_size,hidden_size, bias=True)\n",
        "\n",
        "    self.CS_input_linear = nn.Linear(input_size,hidden_size,bias=True)   \n",
        "    self.CS_hidden_linear = nn.Linear(hidden_size,hidden_size, bias=True)\n",
        "\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    \n",
        "\n",
        "  def forward(self, input_tensor,hidden,prev_cell_state):\n",
        "    forget=self.sigmoid(self.FG_input_linear(input_tensor)+self.FG_hidden_linear(hidden))\n",
        "\n",
        "    input=self.sigmoid(self.IG_input_linear(input_tensor)+self.IG_hidden_linear(hidden))\n",
        "\n",
        "    output=self.sigmoid(self.OG_input_linear(input_tensor)+self.OG_hidden_linear(hidden))\n",
        "\n",
        "    cell_state=self.tanh(self.CS_input_linear(input_tensor)+self.CS_hidden_linear(hidden))\n",
        "\n",
        "    cell_state=prev_cell_state*forget+input*cell_state\n",
        "\n",
        "    hidden=output*self.tanh(cell_state)\n",
        "\n",
        "    return hidden,cell_state\n",
        "\n",
        "\n",
        "class LSTMlayer(nn.Module):\n",
        "  def __init__(self,module):\n",
        "    super(LSTMlayer,self).__init__()\n",
        "    self.lstm=module\n",
        "    self.out_size=64\n",
        "    \n",
        "\n",
        "  def forward(self,input_tensor):\n",
        "  \n",
        "    cur_hidden=torch.zeros(input_tensor.shape[0],input_tensor.shape[2])        #input이 3차원으로 들어오면 (batch, sequence length, embedding d) 여기선 (16,16,64)\n",
        "    cur_state=torch.zeros(input_tensor.shape[0],input_tensor.shape[2])  \n",
        "   \n",
        "    #if input_tensor.is_cuda:\n",
        "      #cur_hidden_tensor = cur_hidden_tensor.cuda()\n",
        "\n",
        "\n",
        "    for i in range(input_tensor.shape[1]):                                      #input이 2차원으로 줄어든다.\n",
        "\n",
        "      cur_input=input_tensor[:,i,:]\n",
        "      cur_hidden,cur_state=self.lstm(cur_input,cur_hidden,cur_state)\n",
        "\n",
        "    return cur_hidden\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMClassification(nn.Module):\n",
        "  def __init__(self,module,d):\n",
        "    super(LSTMClassification,self).__init__()\n",
        "    self.d=d\n",
        "    self.linear=nn.Linear(d,2,bias=True)\n",
        "    self.embedding = nn.Embedding(len(new_vocab2), d)\n",
        "    self.layer=module\n",
        "    \n",
        "\n",
        "  def forward(self,input_tensor):\n",
        "    emb = self.embedding(input_tensor)\n",
        "    outputs=self.layer(emb)\n",
        "    \n",
        "    \n",
        "    outputs=self.linear(emb)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "lstm=LSTM(64,64,64)\n",
        "lstm_layer=LSTMlayer(lstm)\n",
        "\n",
        "lstmclassification=LSTMClassification(lstm_layer,64)\n",
        "\n",
        "dataset = TensorDataset(input_tensor, labels_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "cel = nn.CrossEntropyLoss()\n",
        "  # We use Adam optimizer. This allows us to not worry about learning rate\n",
        "optimizer = torch.optim.Adam(baseline.parameters(), lr=0.0001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    \n",
        "    x_train, y_train = samples[0], samples[1]\n",
        "    #print(samples[0].shape)\n",
        "    #print(samples[1].shape)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs=lstmclassification(x_train)\n",
        "    outputs=outputs[:,0] \n",
        "    print(outputs)\n",
        "    y_train=y_train[:,0] \n",
        "    print(y_train)                                   #차원축소 해줘야함\n",
        "    loss=cel(outputs,y_train)\n",
        "    print('LOSS:',loss)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "testset = TensorDataset(test_input_tensor, test_labels_tensor)\n",
        "test_dataloader = DataLoader(testset, batch_size=16)\n",
        "correct_count=0\n",
        "total_count=0\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        x_test,y_test = data[0], data[1]\n",
        "        test_outputs = lstmclassification(x_test)\n",
        "        test_outputs=test_outputs[:,0] \n",
        "        _, preds = test_outputs.max(1)\n",
        "        y_test=y_test[:,0]   #행을 기준으로 큰 값을 갖는 인덱스 반환\n",
        "        print('y_test:',y_test)\n",
        "        print('preds',preds)\n",
        "        num_correct = (y_test == preds).long().sum()\n",
        "        total_count += x_test.size(0)\n",
        "        correct_count += num_correct\n",
        "        \n",
        "        loss = cel(test_outputs, y_test)\n",
        "        print(loss)\n",
        "        print('correct count: ',num_correct)\n",
        "        print('size:',x_test.size(0)) \n",
        "\n",
        "        \n",
        "    #print('correct count: ',correct_count)\n",
        "    #print('total:',total_count)    \n",
        "    print(\"accuracy:\",correct_count/total_count)     "
      ],
      "metadata": {
        "id": "WyRcq4B0xusi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}